{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. 셀의 코드를 검토하여 오류의 가능한 원인을 식별하세요. 자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'> 여기 </a> 를 클릭하세요. 자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "# 자연어 처리 \n",
    "#import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 13:05:13.675898: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# 1. 전처리 \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do',\n",
       " \"n't\",\n",
       " 'be',\n",
       " 'fooled',\n",
       " 'by',\n",
       " 'the',\n",
       " 'dark',\n",
       " 'sounding',\n",
       " 'name',\n",
       " ',',\n",
       " 'Mr.',\n",
       " 'Jone',\n",
       " \"'s\",\n",
       " 'Orphanage',\n",
       " 'is',\n",
       " 'as',\n",
       " 'cheery',\n",
       " 'as',\n",
       " 'cheery',\n",
       " 'goes',\n",
       " 'for',\n",
       " 'a',\n",
       " 'pastry',\n",
       " 'shop',\n",
       " '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰화\n",
    "#  1.1 word_tokenize (단어 단위로 분해)\n",
    "word_tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\")\n",
    "#  Don't를 Do와 n't로 분리하였으며, 반면 Jone's는 Jone과's로 분리한 것을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'be',\n",
       " 'fooled',\n",
       " 'by',\n",
       " 'the',\n",
       " 'dark',\n",
       " 'sounding',\n",
       " 'name',\n",
       " ',',\n",
       " 'Mr',\n",
       " '.',\n",
       " 'Jone',\n",
       " \"'\",\n",
       " 's',\n",
       " 'Orphanage',\n",
       " 'is',\n",
       " 'as',\n",
       " 'cheery',\n",
       " 'as',\n",
       " 'cheery',\n",
       " 'goes',\n",
       " 'for',\n",
       " 'a',\n",
       " 'pastry',\n",
       " 'shop',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.2 WordPunctTokenizer\n",
    "WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\")\n",
    "\n",
    "# WordPunctTokenizer는 구두점을 별도로 분류하는 특징을 갖고 있기때문에,\n",
    "# 앞서 확인했던 word_tokenize와는 달리 Don't를 Don과'와 t로 분리하였으며,\n",
    "# 이와 마찬가지로 Jone's를 Jone과'와 s로 분리한 것을 확인할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"don't\",\n",
       " 'be',\n",
       " 'fooled',\n",
       " 'bythe',\n",
       " 'dark',\n",
       " 'soundingname',\n",
       " 'mr',\n",
       " \"jone'sorphanage\",\n",
       " 'is',\n",
       " 'ascheery',\n",
       " 'as',\n",
       " 'cheery',\n",
       " 'goesfor',\n",
       " 'a',\n",
       " 'pastry',\n",
       " 'shop']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.3 text_to_word_sequence\n",
    "text_to_word_sequence(\"Don't be fooled bythe dark soundingname, Mr. Jone'sOrphanage is ascheery as cheery goesfor a pastry shop.\")\n",
    "\n",
    "# 케라스의 text_to_word_sequence는 기본적으로 모든 알파벳을 소문자로 바꾸면서 마침표나 컴마, 느낌표 등의 구두점을 제거\n",
    "#  하지만 don't나 jone's와 같은 경우 아포스트로피는 보존하는 것을 볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do',\n",
       " \"n't\",\n",
       " 'be',\n",
       " 'fooled',\n",
       " 'by',\n",
       " 'the',\n",
       " 'dark',\n",
       " 'sounding',\n",
       " 'name',\n",
       " ',',\n",
       " 'Mr.',\n",
       " 'Jone',\n",
       " \"'s\",\n",
       " 'Orphanage',\n",
       " 'is',\n",
       " 'as',\n",
       " 'cheery',\n",
       " 'as',\n",
       " 'cheery',\n",
       " 'goes',\n",
       " 'for',\n",
       " 'a',\n",
       " 'pastry',\n",
       " 'shop.',\n",
       " 'Starting',\n",
       " 'a',\n",
       " 'home-based',\n",
       " 'restaurant',\n",
       " 'may',\n",
       " 'be',\n",
       " 'an',\n",
       " 'ideal',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 표준으로 쓰이고 있는 토큰화 방법 중 하나인 Penn Treebank Tokenization\n",
    "# • 규칙 1. 하이푼으로 구성된 단어는 하나로 유지한다.\n",
    "# • 규칙 2. doesn't와 같이 아포스트로피로'접어'가 함께하는 단어는 분리해준다.\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "text = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop. Starting a home-based restaurant may be an ideal.\"\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['His barber kept his word.',\n",
       " 'But keeping such a huge secret to himself was driving him crazy.',\n",
       " 'Finally, the barber went up a mountain and almost to the edge of a cliff.',\n",
       " 'He dug a hole in the midst of some reeds.',\n",
       " 'He looked about, to make sure no one was near.I am actively looking for Ph.D. students.',\n",
       " 'And you are a Ph.D student.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.4 문장 토큰화 (토큰의 단위가 문장(sentence)일 경우)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
    "text +=\"I am actively looking for Ph.D. students. And you are a Ph.D student.\"\n",
    "\n",
    "sent_tokenize(text)\n",
    "# 직관적으로 생각해봤을 때는 ?나 마침표(.)나 ! 기준으로 문장을 잘라내면 되지 않을까라고 생각할 수 있지만, 꼭 그렇지만은 않음.\n",
    "#  !나 ?는 문장의 구분을 위한 꽤 명확한 구분자(boundary) 역할을 하지만 마침표는 그렇지 않기 때문 EX) Ph.D , Mr. Kim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정제(Cleaning) and 정규화(Normalization)\n",
    "# 정제(cleaning) : 갖고 있는 데이터로부터 노이즈 데이터를 제거한다.\n",
    "#  정규화(normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어준다.\n",
    "#  정제 작업은 토큰화 작업에 방해가 되는 부분들을 배제시키고 토큰화 작업을 수행하기 위해서 토큰화 작업보다 앞서 이루어지기도 하지만, 토큰화 작업 이후에도\n",
    "# 여전히 남아있는 노이즈들을 제거하기위해 지속적으로 이루어지기도 함\n",
    "#  사실 완벽한 정제 작업은 어려운 편이라서, \"대부분의 경우 이 정도면 됐다.”라는 일종의 합의점을 찾기도 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 규칙에 기반한 표기가 다른 단어들의 통합\n",
    "#  필요에 따라 직접 코딩을 통해 정의할 수 있는 정규화 규칙의 예로서 같은 의미를 갖고있음에도, 표기가 다른 단어들을 하나의 단어로 정규화하는 방법을 사용\n",
    "# • Ex)USA와 US는 같은 의미를 가지므로 하나의 단어로 정규화\n",
    "# • Uh-huh와 uhhuh는 형태는 다르지만 여전히 같은 의미\n",
    "# ① 표제어 추출(lemmatization)\n",
    "# ② 어간 추출(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  표제어(Lemma)는 한글로는'표제어' 또는'기본 사전형 단어' 정도의 의미\n",
    "#  표제어 추출은 단어들이 다른 형태를 가지더라도, 그 뿌리 단어를 찾아가서 단어의 개수를 줄일 수 있는지 판단\n",
    "#  예를 들어서 am, are, is는 서로 다른 스펠링이지만 그 뿌리 단어는 be라고 볼 수 있음-> 이때, 이 단어들의 표제어는 be라고 합니다.\n",
    "#  표제어 추출을 하는 가장 섬세한 방법은 단어의 형태학적 파싱을 먼저 진행하는 것\n",
    "# • 어간(stem) : 단어의 의미를 담고 있는 단어의 핵심 부분.\n",
    "# • 접사(affix) : 단어에 추가적인 의미를 주는 부분."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표제어 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "표제어 추출 후 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "# 표제어 추출을 위한 도구인 WordNetLemmatizer함수 사용\n",
    "#  표제어 추출기(lemmatizer)가 본래 단어의 품사 정보를 알아야만 정확한 결과를 얻음\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()  # 인스턴스 생성 \n",
    "\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "\n",
    "print('표제어 추출 전 :',words)\n",
    "print('표제어 추출 후 :',[lemmatizer.lemmatize(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('dies', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watch'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('watched', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('has', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어간 추출은 형태학적 분석을 단순화한 버전이라고 볼 수도 있고, 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업이라고 볼 수도 있음\n",
    "#  어간 추출 속도는 표제어 추출보다 일반적으로 빠른데, 포터 어간 추출기는 정밀하게 설계되어 정확도가 높으므로 영어 자연어 처리에서 어간 추출을 하고자 한다\n",
    "# 면 가장 준수한 선택\n",
    "# • 포터 알고리즘\n",
    "# • 랭커스터 알고리즘\n",
    "# --------------------------------- 한번 집고 가자 --------------------------------------------\n",
    "# 어간 추출은 단어의 형태를 간단히 줄이는 데 유용하며, 텍스트의 문맥적 의미가 크게 중요하지 않은 경우에 적합합니다.\n",
    "# 표제어 추출은 단어의 문법적 의미를 유지하며, 텍스트의 정확한 분석이 필요한 경우에 적합합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "포터 스테머의 어간 추출 후: ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n",
      "랭커스터 스테머의 어간 추출 후: ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print('어간 추출 전 :', words)\n",
    "print('포터 스테머의 어간 추출 후:',[porter_stemmer.stem(w) for w in words])\n",
    "print('랭커스터 스테머의 어간 추출 후:',[lancaster_stemmer.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) 대, 소문자 통합 \n",
    "# 예외 사항을 크게 고려하지 않고, 모든 문장을 소문자로 바꿈.\n",
    "# 3) 불필요한 단어 제거 \n",
    "# 정제 작업에서 제거해야하는 노이즈 데이터(noise data)는 자연어가 아니면서 아무 의미도 갖지 않는 글자들(특수 문자 등)을 의미하기도 하지만, 분석하고자 \n",
    "# 하는 목적에 맞지 않는 불필요 단어들을 노이즈 데이터라고 하기도 함\n",
    "# ① 불용어 제거\n",
    "# ② 길이가 짧은 단어들을 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거 \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "불용어 10개 출력 : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "# NLTK에서는 위와 같은 100여개 이상의 영어 단어들을 불용어로 패키지 내에서 미리 정의\n",
    "stop_words_list = stopwords.words('english')\n",
    "print('불용어 개수 :', len(stop_words_list))\n",
    "print('불용어 10개 출력 :',stop_words_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전: ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "불용어 제거 후: ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
     ]
    }
   ],
   "source": [
    "# 예시 문장\n",
    "example = \"Family is not an important thing. It's everything.\"\n",
    "\n",
    "# 영어 불용어 목록을 가져와서 set(집합)으로 변환\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 예시 문장을 단어 단위로 토큰화\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "# 불용어를 제거한 결과를 저장할 리스트 초기화\n",
    "result = []\n",
    "# 토큰화된 단어들을 하나씩 검사\n",
    "for word in word_tokens: \n",
    "    # 단어가 불용어 목록에 없으면 결과 리스트에 추가\n",
    "    if word not in stop_words: \n",
    "        result.append(word) \n",
    "\n",
    "# 결과 출력\n",
    "print(\"불용어 제거 전:\", word_tokens)\n",
    "print(\"불용어 제거 후:\", result)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was wondering anyone out there could enlighten this car.\n"
     ]
    }
   ],
   "source": [
    "import re  # 정규 표현식을 사용하기 위해 re 모듈을 import\n",
    "\n",
    "# 처리할 텍스트\n",
    "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
    "\n",
    "# 길이가 1~2인 단어들을 매칭하는 정규 표현식 컴파일\n",
    "# r'\\W*\\b\\w{1,2}\\b' : raw string 리터럴을 사용하여 정규 표현식을 작성\n",
    "# \\W* : 단어가 아닌 문자 (공백, 구두점 등)와 매칭\n",
    "# \\b : 단어 경계를 나타냄\n",
    "# \\w{1,2} : 길이가 1~2인 단어와 매칭\n",
    "# \\b : 단어 경계를 나타냄\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "\n",
    "# 정규 표현식을 사용하여 길이가 1~2인 단어들을 삭제하고 결과 출력\n",
    "print(shortword.sub('', text))\n",
    "\n",
    "# 사실 길이가 짧은 단어를 제거하는 2차 이유는 길이를 조건으로 텍스트를 삭제하면서 단어가 아닌 구두점들까지도 한꺼번에 제거하기 위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW 모델 (자연어 메트릭스로 바꿔주는거) - 카운트 기반의 단어 표현 기법\n",
    "# Bag of Words란 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도\n",
    "# (frequency)에만 집중하는 텍스트 데이터의 수치화 표현 방법\n",
    "#  1. 전체 문서에 대해 고유한 토큰(token), 예를 들어 단어로 이루어진 어휘 사전 (vocabulary)을 만듦\n",
    "#  2. 특정 문서에 각 단어가 얼마나 자주 등장하는지 헤아려 문서의 특성 벡터를 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector : [[1 1 2 1 2 1]]\n",
      "vocabulary : {'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "# 사이킷 런에서는 단어의 빈도를 Count하여 Vector로 만드는 CountVectorizer 클래스를 지원\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = ['you know I want your love. because I love you.']\n",
    "vector = CountVectorizer()\n",
    "\n",
    "# 텍스트로부터 각 단어의 빈도수를 기록\n",
    "print('bag of words vector :', vector.fit_transform(text).toarray()) \n",
    "\n",
    "# 각 단어의 인덱스가 어떻게 부여되었는지를 출력\n",
    "print('vocabulary :',vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector : [[1 1 0 0 1 0]\n",
      " [0 0 1 2 0 1]]\n",
      "vocabulary : {'family': 0, 'important': 1, 'thing': 4, 'know': 2, 'want': 5, 'love': 3}\n"
     ]
    }
   ],
   "source": [
    "text = [\"Family is not an important thing. It's everything.\",\n",
    "        'you know I want your love. because I love you.']\n",
    "vect = CountVectorizer(stop_words=\"english\")    # 영어 불용어를 자동으로 제거\n",
    "print('bag of words vector :',vect.fit_transform(text).toarray())    \n",
    "print('vocabulary :',vect.vocabulary_)\n",
    "\n",
    "\n",
    "# fit_transform: 텍스트 데이터를 처음 처리할 때 사용되며, 어휘 사전을 학습하고 데이터를 변환\n",
    "# transform: 이미 학습된 어휘 사전을 사용하여 새로운 데이터를 변환 / 어휘 사전을 재학습하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 단어 행렬(Document-Term Matrix, DTM)\n",
    "# 서로 다른 문서들의 BoW들을 결합한 표현 방법인 문서 단어 행렬(Document-Term Matrix, DTM) 표현 방법\n",
    "# 각 문서에 대한 BoW를 하나의 행렬로 만든 것\n",
    "# 1) 희소 표현(Sparse representation)\n",
    "# • 대부분의 값이 0인 표현을 희소 벡터(sparse vector) 또는 희소 행렬(sparse matrix)라고 부르는데, 희소 벡터는 많은 양의 저장 공간과 높은 계산 복잡도를 요구\n",
    "# • 이러한 이유로 전처리를 통해 단어 집합의 크기를 줄이는 일은 BoW 표현을 사용하는 모델에서 중요\n",
    "# • 앞서 배운 텍스트 전처리 방법을 사용하여 구두점, 빈도수가 낮은 단어, 불용어를 제거하고, 어간이나 표제어 추출을 통해 단어를 정규화하여 단어 집합의 크기를 줄임\n",
    "#  2) 단순 빈도 수 기반 접근\n",
    "# • 불용어(stopwords)와 같은 단어들은 빈도수가 높더라도 자연어 처리에 있어 의미를 갖지 못하는 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF(Term Frequency-Inverse Document Frequency)\n",
    "#  DTM 내에 있는 각 단어에 대한 중요도를 계산할 수 있는 TF-IDF 가중치\n",
    "#  TF-IDF를 사용하면, 기존의 DTM을 사용하는 것보다 보다 많은 정보를 고려하여 문서들을 비교할 수 있음\n",
    "#  TF-IDF가 DTM보다 항상 좋은 성능을 보장하는 것은 아니지만, 많은 경우에서 DTM보다 더 좋은 성능을 얻을 수 있음\n",
    "#  TF-IDF는 단어의 빈도와 역 문서 빈도(문서의 빈도에 특정 식을 취함)를 사용하여 DTM 내의 각 단어들마다 중요한 정도를 가중치로 주는 방법\n",
    "\n",
    "# • tf(d,t) : 특정 문서 d에서의 특정 단어 t의 등장 횟수.\n",
    "# • df(t) : 특정 단어 t가 등장한 문서의 수.\n",
    "# • idf(t) : df(t)에 반비례하는 수.\n",
    "# • N: 문서의 총 개수\n",
    "\n",
    "#  불용어 등과 같이 자주 쓰이는 단어들은 비교적 자주 쓰이지 않는 단어들보다 최소 수십 배 자주 등장\n",
    "#  이 때문에 log를 씌워주지 않으면, 희귀 단어들에 엄청난 가중치가 부여될 수 있음\n",
    "#  로그를 씌우면 이런 격차를 줄이는 효과가 있음\n",
    "#  log 안의 식에서 분모에 1을 더해주는 이유는 첫번째 이유로는 특정 단어가 전체 문서에서 등장하지 않을 경우에 분모가 0이 되는 상황을 방지하기 위함입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 1 0 1 1]\n",
      " [0 0 1 0 0 0 0 1 0]\n",
      " [1 0 0 0 1 0 1 0 0]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "#  TF-IDF는 모든 문서에서 자주 등장하는 단어는 중요도가 낮다고 판단\n",
    "# • 모든 문서에서 자주 등장하는 단어->df(t)값이 높아짐->최종 점수에는 반비례하여 적용\n",
    "# • 즉, the나 a와 같이 불용어의 경우에는 모든 문서에 자주 등장하기 마련이기 때문에 자연스럽게 불용어의 TF-IDF의 값은 다른 단어의 TF-IDF에 비해서 낮아지게 됩니다.\n",
    "#  특정 문서에서만 자주 등장하는 단어는 중요도가 높다고 판단\n",
    "# • 특정 문서에서만 자주 등장하는 단어->tf(t,d)가 높음\n",
    "#  TF-IDF 값이 낮으면 중요도가 낮은 것이며, TF-IDF 값이 크면 중요도가 큰 것\n",
    "\n",
    "# 사이킷런을 이용한 TF-IDF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do ',    \n",
    "]\n",
    "\n",
    "vector = CountVectorizer()\n",
    "\n",
    "# 코퍼스로부터 각 단어의 빈도수를 기록\n",
    "print(vector.fit_transform(corpus).toarray())\n",
    "\n",
    "# 각 단어와 맵핑된 인덱스 출력\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
      "  0.         0.35543247 0.46735098]\n",
      " [0.         0.         0.79596054 0.         0.         0.\n",
      "  0.         0.60534851 0.        ]\n",
      " [0.57735027 0.         0.         0.         0.57735027 0.\n",
      "  0.57735027 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do ',    \n",
    "]\n",
    "# TfidfVectorizer 객체 생성 및 코퍼스 학습\n",
    "# tfidfv = TfidfVectorizer().fit(corpus)\n",
    "\n",
    "# 코퍼스를 TF-IDF 벡터로 변환하여 배열로 출력\n",
    "# print(tfidfv.transform(corpus).toarray())\n",
    "# ----------------------------------------------------------------------( 왜? 위에는 재사용성을 높이기 위해 밑에는 코드의 간결화 )\n",
    "# TfidfVectorizer 객체 생성\n",
    "tfidfv = TfidfVectorizer()\n",
    "\n",
    "# fit_transform을 사용하여 텍스트 데이터를 한 번에 학습하고 변환\n",
    "tfidf_matrix = tfidfv.fit_transform(corpus).toarray()\n",
    "\n",
    "# 결과 출력\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  4. IMDB 리뷰 감성 분류하기 (데이터 로드 )\n",
    "# 이 마스(Maas) 등이 수집한 대규모 IMDb의 영화 리뷰 데이터셋을 사용\n",
    "#  영화 리뷰 데이터셋은 긍정(1) 또는 부정(0)으로 레이블되어 있는 영화 리뷰 5만개로 구성\n",
    "# • 긍정: IMDb에서 별 여섯 개 이상을 받은 영화를 말함\n",
    "# • 부정: IMDb에서 별 다섯개 아래를 받은 영화를 말함\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/Users/mac/Desktop/24년 대학/1학기/머신러닝 프로그래밍/강의코드/gihorse/movie_data.csv', encoding='utf-8')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37500,), (12500,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 데이터 분리\n",
    "#  바로 train데이터와 test데이터 분리\n",
    "#  Why? 실제 사용자나 어플리케이션의 데이터는 전처리 없는 데이터를 모델에 input함\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = df[\"review\"]\n",
    "target = df[\"sentiment\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0.25, shuffle=True, stratify=target, random_state=34)\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 - 불필요한 단어 제거 \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocessor(text):\n",
    "    \n",
    "    # HTML 태그 제거\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    \n",
    "     # 숫자 제거\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # 길이가 1~2인 단어들을 매칭하는 정규 표현식 컴파일 및 제거 \n",
    "    shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "    text=shortword.sub('', text)\n",
    "    \n",
    "    # 이모티콘을 찾아 리스트로 저장\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    \n",
    "    # 텍스트를 소문자로 변환하고, 비문자(non-word) 문자를 공백으로 대체\n",
    "    # 이모티콘을 공백을 두고 텍스트에 추가, '-'는 제거\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"Murder in Greenwich\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 적용 전 \n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the teenager martha moxley maggie grace moves the high class area belle haven greenwich connecticut the mischief night eve halloween she was murdered the backyard her house and her murder remained unsolved twenty two years later the writer mark fuhrman christopher meloni who former detective that has fallen disgrace for perjury simpson trial and moved idaho decides investigate the case with his partner stephen weeks andrew mitchell with the purpose writing book the locals squirm and not welcome them but with the support the retired detective steve carroll robert forster that was charge the investigation the they discover the criminal and net power and money cover the murder murder greenwich good movie with the true story murder fifteen years old girl that was committed wealthy teenager whose mother was kennedy the powerful and rich family used their influence cover the murder for more than twenty years however snoopy detective and convicted perjurer disgrace was able disclose how the hideous crime was committed the screenplay shows the investigation mark and the last days martha parallel but there lack the emotion the dramatization vote seven title brazil not available'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 적용 후 \n",
    "x_train = x_train.apply(preprocessor)\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37500, 89561)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DTM 생성\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(stop_words=\"english\") # 불용어 제거 후\n",
    "x_clean = vect.fit_transform(x_train).toarray() # DTM 생성\n",
    "x_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로지스틱 모델로 학습\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(fit_intercept=True) # fit_intercept=True는 모델이 데이터의 절편을 학습\n",
    "logreg.fit(x_clean, y_train)  # x_clean은 전처리된 입력 데이터(특징 벡터)이며, y_train은 해당 데이터의 레이블(타겟)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_dtm= vect.transform(x_test).toarray()\n",
    "x_test_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = logreg.predict(x_test_dtm)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred) # 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=[\"The movie was great!\"]\n",
    "new_dtm= vect.transform(text).toarray()\n",
    "logreg.predict(new_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=[\"I did not like the movie. It was boring.\"]\n",
    "new_dtm= vect.transform(text).toarray()\n",
    "logreg.predict(new_dtm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "24ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
